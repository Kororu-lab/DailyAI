[
  {
    "title": "Data centers love solar: Here’s a comprehensive guide to deals over 100 megawatts",
    "url": "https://techcrunch.com/2025/03/30/data-centers-love-solar-heres-a-comprehensive-guide-to-deals-over-100-megawatts/",
    "author": "Tim De Chant",
    "date": "2025-04-01",
    "body": "New and expanded data centers are expected to double the sector’s power demand by 2029 as tech companies rush to capitalize on AI.",
    "source": "TechCrunch AI"
  },
  {
    "title": "Nvidia thinks AI can solve electrical grid problems caused by AI",
    "url": "https://techcrunch.com/2025/03/20/nvidia-thinks-ai-can-solve-electrical-grid-problems-caused-by-ai/",
    "author": "Tim De Chant",
    "date": "2025-04-01",
    "body": "The Open Power AI Consortium says it will use domain-specific AI models to tackle problems in the power industry.",
    "source": "TechCrunch AI"
  },
  {
    "title": "Solar notches another win as Microsoft adds 475 MW to power its AI data centers",
    "url": "https://techcrunch.com/2025/03/20/solar-notches-another-win-as-microsoft-adds-475-mw-to-power-its-ai-data-centers/",
    "author": "Tim De Chant",
    "date": "2025-04-01",
    "body": "The company recently signed a deal with energy provider AES for three solar projects across the Midwest.",
    "source": "TechCrunch AI"
  },
  {
    "title": "Geothermal could power nearly all new data centers through 2030",
    "url": "https://techcrunch.com/2025/03/11/geothermal-could-power-nearly-all-new-data-centers-through-2030/",
    "author": "Tim De Chant",
    "date": "2025-04-01",
    "body": "Geothermal resources have enormous potential to provide the sort of consistent power that data centers crave.",
    "source": "TechCrunch AI"
  },
  {
    "title": "ElevenLabs now lets authors create and publish audiobooks on its own platform",
    "url": "https://techcrunch.com/2025/02/25/elevenlabs-is-now-letting-authors-create-and-publish-audiobooks-on-its-own-platform/",
    "author": "Ivan Mehta",
    "date": "2025-04-01",
    "body": "Voice AI company ElevenLabs is now letting authors publish AI-generated audiobooks on its own Reader app, TechCrunch has learned and the company confirmed. The announcement comes days after the company partnered with Spotify for AI-narrated audiobooks. ElevenLabs, which raised a $180 million mega-round last month, started inviting authors to try out their publishing program through [&#8230;]",
    "source": "TechCrunch AI"
  },
  {
    "title": "Data center tweaks could unlock 76 GW of new power capacity in the US",
    "url": "https://techcrunch.com/2025/02/13/data-center-tweaks-could-unlock-76-gw-of-new-power-capacity-in-the-u-s/",
    "author": "Tim De Chant",
    "date": "2025-04-01",
    "body": "A new study argues that data centers could be ideal demand-response participants because they have the potential to be flexible.",
    "source": "TechCrunch AI"
  },
  {
    "title": "YouTube AI updates include auto dubbing expansion, age ID tech, and more",
    "url": "https://techcrunch.com/2025/02/11/youtube-ai-updates-to-include-expansion-of-auto-dubbing-age-identifying-tech-and-more/",
    "author": "Sarah Perez",
    "date": "2025-04-01",
    "body": "In his annual letter, YouTube CEO Neal Mohan dubbed AI one of the company&#8217;s four &#8220;big bets&#8221; for 2025. The executive pointed to the company&#8217;s investments in AI tools for creators, including ones for video ideas, thumbnails, and language translation. The latter feature will roll out to all creators in YouTube&#8217;s Partner Program this month, [&#8230;]",
    "source": "TechCrunch AI"
  },
  {
    "title": "Self Inspection raises $3M for its AI-powered vehicle inspections",
    "url": "https://techcrunch.com/2025/02/07/self-inspection-raises-3m-for-its-ai-powered-vehicle-inspections/",
    "author": "Sean O'Kane",
    "date": "2025-04-01",
    "body": "A number of startups are racing to make vehicle inspections faster, easier, and cheaper. Self Inspection, a startup based in San Diego, thinks it has them all beat with its AI-powered service &#8212; and now it has convinced outside investors. Self Inspection, founded in 2021, is set to announce Thursday it&#8217;s raised $3 million in [&#8230;]",
    "source": "TechCrunch AI"
  },
  {
    "title": "Meta turns to solar — again — in its data center-building boom",
    "url": "https://techcrunch.com/2025/01/31/meta-turns-to-solar-again-in-its-data-center-building-boom/",
    "author": "Tim De Chant",
    "date": "2025-04-01",
    "body": "The announcement comes as Meta CEO Mark Zuckerberg maintains the company’s ambitious AI strategy, which will require hefty capital investments in data centers.",
    "source": "TechCrunch AI"
  },
  {
    "title": "How to switch off Apple Intelligence on your iPhone, iPad, and Mac",
    "url": "https://techcrunch.com/2025/01/27/how-to-switch-off-apple-intelligence-on-your-iphone-ipad-and-mac/",
    "author": "Lorenzo Franceschi-Bicchierai",
    "date": "2025-04-01",
    "body": "Here's a step-by-step guide on how to turn off and disable Apple Intelligence from your devices.",
    "source": "TechCrunch AI"
  },
  {
    "title": "The first trial of generative AI therapy shows it might help with depression",
    "url": "https://www.technologyreview.com/2025/03/28/1114001/the-first-trial-of-generative-ai-therapy-shows-it-might-help-with-depression/",
    "author": "James O'Donnell",
    "date": "2025-04-01",
    "body": "The first clinical trial of a therapy bot that uses generative AI suggests it was as effective as human therapy for participants with depression, anxiety, or risk for developing eating disorders. Even so, it doesn’t give a go-ahead to the dozens of companies hyping such technologies while operating in a regulatory gray area.&#160; A team&#8230;",
    "source": "MIT Technology Review AI"
  },
  {
    "title": "Anthropic can now track the bizarre inner workings of a large language model",
    "url": "https://www.technologyreview.com/2025/03/27/1113916/anthropic-can-now-track-the-bizarre-inner-workings-of-a-large-language-model/",
    "author": "Will Douglas Heaven",
    "date": "2025-04-01",
    "body": "The AI firm Anthropic has developed a way to peer inside a large language model and watch what it does as it comes up with a response, revealing key new insights into how the technology works. The takeaway: LLMs are even stranger than we thought. The Anthropic team was surprised by some of the counterintuitive&#8230;",
    "source": "MIT Technology Review AI"
  },
  {
    "title": "Google’s new experimental Gemini 2.5 model rolls out to free users",
    "url": "https://arstechnica.com/gadgets/2025/03/googles-new-experimental-gemini-2-5-model-rolls-out-to-free-users/",
    "author": "Ryan Whitwam",
    "date": "2025-04-01",
    "body": "Google's improved AI model is now available for free, but usage is limited.",
    "source": "Ars Technica AI"
  },
  {
    "title": "Gemini hackers can deliver more potent attacks with a helping hand from… Gemini",
    "url": "https://arstechnica.com/security/2025/03/gemini-hackers-can-deliver-more-potent-attacks-with-a-helping-hand-from-gemini/",
    "author": "Dan Goodin",
    "date": "2025-04-01",
    "body": "Hacking LLMs has always been more art than science. A new attack on Gemini could change that.",
    "source": "Ars Technica AI"
  },
  {
    "title": "Google announces Maps screenshot analysis, AI itineraries to help you plan trips",
    "url": "https://arstechnica.com/gadgets/2025/03/google-announces-maps-screenshot-analysis-ai-itineraries-to-help-you-plan-trips/",
    "author": "Ryan Whitwam",
    "date": "2025-04-01",
    "body": "Google wants to help you get away this summer with, you guessed it, AI.",
    "source": "Ars Technica AI"
  },
  {
    "title": "Gemini 2.5 Pro is here with bigger numbers and great vibes",
    "url": "https://arstechnica.com/ai/2025/03/google-says-the-new-gemini-2-5-pro-model-is-its-smartest-ai-yet/",
    "author": "Ryan Whitwam",
    "date": "2025-04-01",
    "body": "Google's new \"thinking\" model is ready to think for you.",
    "source": "Ars Technica AI"
  },
  {
    "title": "Mom horrified by Character.AI chatbots posing as son who died by suicide",
    "url": "https://arstechnica.com/tech-policy/2025/03/mom-horrified-by-character-ai-chatbots-posing-as-son-who-died-by-suicide/",
    "author": "Ashley Belanger",
    "date": "2025-04-01",
    "body": "Character.AI takes down bots bearing likeness of boy at center of lawsuit.",
    "source": "Ars Technica AI"
  },
  {
    "title": "Judge disses Star Trek icon Data’s poetry while ruling AI can’t author works",
    "url": "https://arstechnica.com/tech-policy/2025/03/judge-disses-star-trek-icon-datas-poetry-while-ruling-ai-cant-author-works/",
    "author": "Ashley Belanger",
    "date": "2025-04-01",
    "body": "Computer scientist won't give up fight to copyright AI-made art after court loss.",
    "source": "Ars Technica AI"
  },
  {
    "title": "Apple promises $500 billion in US investment in wake of tariff threats",
    "url": "https://arstechnica.com/tech-policy/2025/02/apple-promises-500-billion-in-us-investment-in-wake-of-tariff-threats/",
    "author": "Ashley Belanger",
    "date": "2025-04-01",
    "body": "Trump tariffs risk spiking costs of both Apple's business and its products.",
    "source": "Ars Technica AI"
  },
  {
    "title": "AI making up cases can get lawyers fired, scandalized law firm warns",
    "url": "https://arstechnica.com/tech-policy/2025/02/ai-making-up-cases-can-get-lawyers-fired-scandalized-law-firm-warns/",
    "author": "Ashley Belanger",
    "date": "2025-04-01",
    "body": "“Nauseatingly frightening”: Law firm condemns careless AI use in court.",
    "source": "Ars Technica AI"
  },
  {
    "title": "Condé Nast, other news orgs say AI firm stole articles, spit out “hallucinations”",
    "url": "https://arstechnica.com/tech-policy/2025/02/news-orgs-accuse-ai-firm-of-stealing-articles-and-creating-fake-news/",
    "author": "Jon Brodkin",
    "date": "2025-04-01",
    "body": "Publishers sue Cohere, say AI firm is \"stealing our works.\"",
    "source": "Ars Technica AI"
  },
  {
    "title": "New hack uses prompt injection to corrupt Gemini’s long-term memory",
    "url": "https://arstechnica.com/security/2025/02/new-hack-uses-prompt-injection-to-corrupt-geminis-long-term-memory/",
    "author": "Dan Goodin",
    "date": "2025-04-01",
    "body": "There's yet another way to inject malicious prompts into chatbots.",
    "source": "Ars Technica AI"
  },
  {
    "title": "Deep dive: How I use robots to survey coral reefs",
    "url": "https://www.nature.com/articles/d41586-025-00936-0",
    "date": "2025-04-01",
    "author": "Nikki Forrester",
    "body": "<p>Nature, Published online: 31 March 2025; <a href=\"https://www.nature.com/articles/d41586-025-00936-0\">doi:10.1038/d41586-025-00936-0</a></p>Marine ecologist Gemma Galbraith builds remotely operated vehicles and uses them to assess how coral reefs are being affected by climate change.",
    "source": "Nature AI"
  },
  {
    "title": "Brain implant translates thoughts to speech in an instant",
    "url": "https://www.nature.com/articles/d41586-025-01001-6",
    "date": "2025-04-01",
    "author": "Miryam Naddaf",
    "body": "<p>Nature, Published online: 31 March 2025; <a href=\"https://www.nature.com/articles/d41586-025-01001-6\">doi:10.1038/d41586-025-01001-6</a></p>Improvements to brain–computer interfaces are bringing the technology closer to natural conversation speed.",
    "source": "Nature AI"
  },
  {
    "title": "Big cuts to US AIDS prevention feared as NIH axes HIV research grants",
    "url": "https://www.nature.com/articles/d41586-025-00969-5",
    "date": "2025-04-01",
    "author": "Humberto Basilio",
    "body": "<p>Nature, Published online: 31 March 2025; <a href=\"https://www.nature.com/articles/d41586-025-00969-5\">doi:10.1038/d41586-025-00969-5</a></p>More than 200 federal grants for research related to HIV and AIDS have been abruptly terminated in the last few weeks.",
    "source": "Nature AI"
  },
  {
    "title": "Style over substance? What birds’ mating behaviours reveal about sexual selection",
    "url": "https://www.nature.com/articles/d41586-025-00934-2",
    "date": "2025-04-01",
    "author": "Tim Coulson",
    "body": "<p>Nature, Published online: 31 March 2025; <a href=\"https://www.nature.com/articles/d41586-025-00934-2\">doi:10.1038/d41586-025-00934-2</a></p>An exploration of weird and wonderful birds across the world sometimes takes theories of sexual selection to the extreme — but entertains throughout.",
    "source": "Nature AI"
  },
  {
    "title": "Audio long read: How quickly are you ageing? What molecular ‘clocks’ can tell you about your health",
    "url": "https://www.nature.com/articles/d41586-025-00984-6",
    "date": "2025-04-01",
    "author": "Benjamin Thompson",
    "body": "<p>Nature, Published online: 28 March 2025; <a href=\"https://www.nature.com/articles/d41586-025-00984-6\">doi:10.1038/d41586-025-00984-6</a></p>Researchers are looking to improve how ageing is measured, but the field is plagued with uncertainties.",
    "source": "Nature AI"
  },
  {
    "title": "‘Open source’ AI isn’t truly open — here’s how researchers can reclaim the term",
    "url": "https://www.nature.com/articles/d41586-025-00930-6",
    "date": "2025-04-01",
    "author": "Stefano Maffulli",
    "body": "<p>Nature, Published online: 27 March 2025; <a href=\"https://www.nature.com/articles/d41586-025-00930-6\">doi:10.1038/d41586-025-00930-6</a></p>Many firms are misusing the ‘open source’ label. The scientific community, which relies on transparency and replicability, must resist this trend.",
    "source": "Nature AI"
  },
  {
    "title": "75% of US scientists who answered <i>Nature</i> poll consider leaving",
    "url": "https://www.nature.com/articles/d41586-025-00938-y",
    "date": "2025-04-01",
    "author": "Alexandra Witze",
    "body": "<p>Nature, Published online: 27 March 2025; <a href=\"https://www.nature.com/articles/d41586-025-00938-y\">doi:10.1038/d41586-025-00938-y</a></p>More than 1,600 readers answered our poll; many said they were looking for jobs in Europe and Canada.",
    "source": "Nature AI"
  },
  {
    "title": "How Trump is following Project 2025’s radical roadmap to defund science",
    "url": "https://www.nature.com/articles/d41586-025-00780-2",
    "date": "2025-04-01",
    "author": "Dan Garisto",
    "body": "<p>Nature, Published online: 27 March 2025; <a href=\"https://www.nature.com/articles/d41586-025-00780-2\">doi:10.1038/d41586-025-00780-2</a></p>Much of the Trump administration’s agenda for research is laid out in the 900-plus-page blueprint. Nature read it so you don’t have to.",
    "source": "Nature AI"
  },
  {
    "title": "Showing ‘ability’ in ‘disability’ — how I mastered interviews while using a wheelchair",
    "url": "https://www.nature.com/articles/d41586-025-00559-5",
    "date": "2025-04-01",
    "author": "Emilia Krok",
    "body": "<p>Nature, Published online: 27 March 2025; <a href=\"https://www.nature.com/articles/d41586-025-00559-5\">doi:10.1038/d41586-025-00559-5</a></p>Learning how to influence the way people see me when I enter a room has been key to boosting my confidence in job interviews.",
    "source": "Nature AI"
  },
  {
    "title": "Daily briefing: Pregnancy’s true toll on the body",
    "url": "https://www.nature.com/articles/d41586-025-00988-2",
    "date": "2025-04-01",
    "author": "Jacob Smith",
    "body": "<p>Nature, Published online: 27 March 2025; <a href=\"https://www.nature.com/articles/d41586-025-00988-2\">doi:10.1038/d41586-025-00988-2</a></p>A huge study paints the most detailed picture yet of the toll pregnancy and childbirth take on the body. Plus, scientists have discovered a new antibiotic in a lab technician’s garden and artificial intelligence tools are making their way into the process of peer review — to some researchers’ dismay.",
    "source": "Nature AI"
  },
  {
    "title": "AlexNet Source Code Is Now Open Source",
    "url": "https://spectrum.ieee.org/alexnet-source-code",
    "author": "Hansen Hsu",
    "date": "2025-04-01",
    "body": "In partnership with Google, the Computer History Museum (CHM) has released the source code to AlexNet, the neural network that in 2012 kickstarted today’s prevailing approach to AI. The source code is available as open source on CHM’s GitHub page. What Is AlexNet? AlexNet is an artificial neural network created to recognize the contents of photographic images. It was developed in 2012 by then–University of Toronto graduate students Alex Krizhevsky and Ilya Sutskever and their faculty advisor, Geoffrey Hinton. The Origins of Deep Learning Hinton is regarded as one of the fathers of deep learning, the type of artificial intelligence that uses neural networks and is the foundation of today’s mainstream AI. Simple three-layer neural networks with only one layer of adaptive weights were first built in the late 1950s—most notably by Cornell researcher Frank Rosenblatt—but they were found to have limitations. [This explainer gives more details on how neural networks work.] In particular, researchers needed networks with more than one layer of adaptive weights, but there wasn’t a good way to train them. By the early 1970s, neural networks had been largely rejected by AI researchers. Frank Rosenblatt [left, shown with Charles W. Wightman] developed the first artificial neural network, the perceptron, in 1957.Division of Rare and Manuscript Collections/Cornell University Library In the 1980s, neural network research was revived outside the AI community by cognitive scientists at the University of California, San Diego, under the new name of “connectionism.” After finishing his Ph.D. at the University of Edinburgh in 1978, Hinton had become a postdoctoral fellow at UCSD, where he collaborated with David Rumelhart and Ronald Williams. The three rediscovered the backpropagation algorithm for training neural networks, and in 1986 they published two papers showing that it enabled neural networks to learn multiple layers of features for language and vision tasks. Backpropagation, which is foundational to deep learning today, uses the difference between the current output and the desired output of the network to adjust the weights in each layer, from the output layer backward to the input layer. In 1987, Hinton joined the University of Toronto. Away from the centers of traditional AI, Hinton’s work and those of his graduate students made Toronto a center of deep learning research over the coming decades. One postdoctoral student of Hinton’s was Yann LeCun, now chief scientist at Meta. While working in Toronto, LeCun showed that when backpropagation was used in “convolutional” neural networks, they became very good at recognizing handwritten numbers. ImageNet and GPUs Despite these advances, neural networks could not consistently outperform other types of machine learning algorithms. They needed two developments from outside of AI to pave the way. The first was the emergence of vastly larger amounts of data for training, made available through the Web. The second was enough computational power to perform this training, in the form of 3D graphics chips, known as GPUs. By 2012, the time was ripe for AlexNet. Fei-Fei Li’s ImageNet image dataset, completed in 2009, was pivotal in training AlexNet. Here, Li [right] talks with Tom Kalil at the Computer History Museum.Douglas Fairbairn/Computer History Museum The data needed to train AlexNet was found in ImageNet, a project started and led by Stanford professor Fei-Fei Li. Beginning in 2006, and against conventional wisdom, Li envisioned a dataset of images covering every noun in the English language. She and her graduate students began collecting images found on the Internet and classifying them using a taxonomy provided by WordNet, a database of words and their relationships to each other. Given the enormity of their task, Li and her collaborators ultimately crowdsourced the task of labeling images to gig workers, using Amazon’s Mechanical Turk platform. Completed in 2009, ImageNet was larger than any previous image dataset by several orders of magnitude. Li hoped its availability would spur new breakthroughs, and she started a competition in 2010 to encourage research teams to improve their image-recognition algorithms. But over the next two years, the best systems only made marginal improvements. The second condition necessary for the success of neural networks was economical access to vast amounts of computation. Neural-network training involves a lot of repeated matrix multiplications, preferably done in parallel, something that GPUs are designed to do. Nvidia, cofounded by CEO Jensen Huang, had led the way in the 2000s in making GPUs more generalizable and programmable for applications beyond 3D graphics, especially with the CUDA programming system released in 2007.Both ImageNet and CUDA were, like neural networks themselves, fairly niche developments that were waiting for the right circumstances to shine. In 2012, AlexNet brought together these elements—deep neural networks, big datasets, and GPUs—for the first time, with pathbreaking results. Each of these needed the other.How AlexNet Was CreatedBy the late 2000s, Hinton’s grad students at the University of Toronto were beginning to use GPUs to train neural networks for both image and speech recognition. Their first successes came in speech recognition, but success in image recognition would point to deep learning as a possible general-purpose solution to AI. One student, Ilya Sutskever, believed that the performance of neural networks would scale with the amount of data available, and the arrival of ImageNet provided the opportunity.A milestone occurred in 2011, when DanNet, a convolutional neural network trained on GPUs created by Dan Cireşan and others at Jürgen Schmidhuber’s lab in Switzerland, won 4 image recognition contests. However, these results were on smaller datasets and weren’t able to move the field of computer vision. ImageNet, which was much larger and more comprehensive, was different. That same year, Sutskever convinced fellow Toronto grad student Alex Krizhevsky, who had a keen ability to wring maximum performance out of GPUs, to train a convolutional neural network for ImageNet, with Hinton serving as principal investigator. AlexNet used Nvidia GPUs running CUDA code trained on the ImageNet dataset. Nvidia CEO Jensen Huang was named a 2024 CHM Fellow for his contributions to computer graphics chips and AI.Douglas Fairbairn/Computer History MuseumKrizhevsky had already written CUDA code for a convolutional neural network using Nvidia GPUs, called cuda-convnet, trained on the much smaller CIFAR-10 image dataset. He extended cuda-convnet with support for multiple GPUs and other features and retrained it on ImageNet. The training was done on a computer with two Nvidia cards in Krizhevsky’s bedroom at his parents’ house. Over the course of the next year, he constantly tweaked the network’s parameters and retrained it until it achieved performance superior to its competitors. The network would ultimately be named AlexNet, after Krizhevsky. Geoff Hinton summed up the AlexNet project this way: “Ilya thought we should do it, Alex made it work, and I got the Nobel prize.” Krizhevsky, Sutskever, and Hinton wrote a paper on AlexNet that was published in the fall of 2012 and presented by Krizhevsky at a computer-vision conference in Florence, Italy, in October. Veteran computer-vision researchers weren’t convinced, but LeCun, who was at the meeting, pronounced it a turning point for AI. He was right. Before AlexNet, almost none of the leading computer-vision papers used neural nets. After it, almost all of them would. AlexNet was just the beginning. In the next decade, neural networks would advance to synthesize believable human voices, beat champion Go players, and generate artwork, culminating with the release of ChatGPT in November 2022 by OpenAI, a company cofounded by Sutskever. Releasing the AlexNet Source Code In 2020, I reached out to Krizhevsky to ask about the possibility of allowing CHM to release the AlexNet source code, due to its historical significance. He connected me to Hinton, who was working at Google at the time. Google owned AlexNet, having acquired DNNresearch, the company owned by Hinton, Sutskever, and Krizhevsky. Hinton got the ball rolling by connecting CHM to the right team at Google. CHM worked with the Google team for five years to negotiate the release. The team also helped us identify the specific version of the AlexNet source code to release—there have been many versions of AlexNet over the years. There are other repositories of code called AlexNet on GitHub, but many of these are re-creations based on the famous paper, not the original code. CHM is proud to present the source code to the 2012 version of AlexNet, which transformed the field of artificial intelligence. You can access the source code on CHM’s GitHub page.This post originally appeared on the blog of the Computer History Museum.This article was updated on 25 March 2025.AcknowledgmentsSpecial thanks to Geoffrey Hinton for providing his quote and reviewing the text, to Cade Metz and Alex Krizhevsky for additional clarifications, and to David Bieber and the rest of the team at Google for their work in securing the source code release.ReferencesFei-Fei Li, The Worlds I See: Curiosity, Exploration, and Discovery at the Dawn of AI. First edition, Flatiron Books, New York, 2023.Cade Metz, Genius Makers: The Mavericks Who Brought AI to Google, Facebook, and the World. First edition, Penguin Random House, New York, 2022.",
    "source": "IEEE Spectrum AI"
  },
  {
    "title": "Synthetic Data Paves the Way for Self-Driving Cars",
    "url": "https://spectrum.ieee.org/synthetic-data-self-driving",
    "author": "Eliza Strickland",
    "date": "2025-04-01",
    "body": "Self-driving cars were supposed to be in our garages by now, according to the optimistic predictions of just a few years ago. But we may be nearing a few tipping points, with robotaxi adoption going up and consumers getting accustomed to more and more sophisticated driver-assistance systems in their vehicles. One company that’s pushing things forward is the Silicon Valley-based Helm.ai, which develops software for both driver-assistance systems and fully autonomous vehicles.The company provides foundation models for the intent prediction and path planning that self-driving cars need on the road, and also uses generative AI to create synthetic training data that prepares vehicles for the many, many things that can go wrong out there. IEEE Spectrum spoke with Vladislav Voroninski, founder and CEO of Helm.ai, about the company’s creation of synthetic data to train and validate self-driving car systems.How is Helm.ai using generative AI to help develop self-driving cars?Vladislav Voroninski: We’re using generative AI for the purposes of simulation. So given a certain amount of real data that you’ve observed, can you simulate novel situations based on that data? You want to create data that is as realistic as possible while actually offering something new. We can create data from any camera or sensor to increase variety in those data sets and address the corner cases for training and validation.I know you have VidGen to create video data and WorldGen to create other types of sensor data. Are different car companies still relying on different modalities?Voroninski: There’s definitely interest in multiple modalities from our customers. Not everyone is just trying to do everything with vision only. Cameras are relatively cheap, while lidar systems are more expensive. But we can actually train simulators that take the camera data and simulate what the lidar output would have looked like. That can be a way to save on costs. And even if it’s just video, there will be some cases that are incredibly rare or pretty much impossible to get or too dangerous to get while you’re doing real-time driving. And so we can use generative AI to create video data that is very, very high-quality and essentially indistinguishable from real data for those cases. That also is a way to save on data collection costs.How do you create these unusual edge cases? Do you say, “Now put a kangaroo in the road, now put a zebra on the road”?Voroninski: There’s a way to query these models to get them to produce unusual situations—it’s really just about incorporating ways to control the simulation models. That can be done with text or prompt images or various types of geometrical inputs. Those scenarios can be specified explicitly: If an automaker already has a laundry list of situations that they know can occur, they can query these foundation models to produce those situations. You can also do something even more scalable where there’s some process of exploration or randomization of what happens in the simulation, and that can be used to test your self-driving stack against various situations.And one nice thing about video data, which is definitely still the dominant modality for self-driving, you can train on video data that is not just coming from driving. So when it comes to those rare object categories, you can actually find them in a lot of different data sets. So if you have a video data set of animals in a zoo, is that going to help a driving system recognize the kangaroo in the road?Voroninski: For sure, that kind of data can be used to train perception systems to understand those different object categories. And it can also be used to simulate sensor data that incorporates those objects into a driving scenario. I mean, similarly, very few humans have seen a kangaroo on a road in real life. Or even maybe in a video. But it’s easy enough to conjure up in your mind, right? And if you do see it, you’ll be able to understand it pretty quickly. What’s nice about generative AI is that if [the model] is exposed to different concepts in different scenarios, it can combine those concepts in novel situations. It can observe it in other situations and then bring that understanding to driving.How do you do quality control for synthetic data? How do you assure your customers that it’s as good as the real thing?Voroninski: There are metrics you can capture that assess numerically the similarity of real data to synthetic data. One example is you take a collection of real data and you take a collection of synthetic data that’s meant to emulate it. And you can fit a probability distribution to both. And then you can compare numerically the distance between those probability distributions.Secondly, we can verify that the synthetic data is useful for solving certain problems. You can say, “We’re going to address this corner case. You can only use simulated data.” You can verify that using the simulated data actually does solve the problem and improve the accuracy on this task without ever training on real data. Are there naysayers who say that synthetic data will never be good enough to train these systems and teach them everything they need to know?Voroninski: The naysayers are typically not AI experts. If you look for where the puck is going, it’s pretty clear that simulation is going to have a huge impact on developing autonomous driving systems. Also, what’s good enough is a moving target, same as the definition of AI or AGI [artificial general intelligence]. Certain developments are made, and then people get used to them, “Oh, that’s no longer interesting. It’s all about this next thing.” But I think it’s pretty clear that AI-based simulation will continue to improve. If you explicitly want an AI system to model something, there’s not a bottleneck at this point. And then it’s just a question of how well it generalizes.",
    "source": "IEEE Spectrum AI"
  },
  {
    "title": "With Gemini Robotics, Google Aims for Smarter Robots",
    "url": "https://spectrum.ieee.org/gemini-robotics",
    "author": "Eliza Strickland",
    "date": "2025-04-01",
    "body": "Generative AI models are getting closer to taking action in the real world. Already, the big AI companies are introducing AI agents that can take care of web-based busywork for you, ordering your groceries or making your dinner reservation. Today, Google DeepMind announced two generative AI models designed to power tomorrow’s robots. The models are both built on Google Gemini, a multimodal foundation model that can process text, voice, and image data to answer questions, give advice, and generally help out. DeepMind calls the first of the new models, Gemini Robotics, an “advanced vision-language-action model,” meaning that it can take all those same inputs and then output instructions for a robot’s physical actions. The models are designed to work with any hardware system, but were mostly tested on the two-armed Aloha 2 system that DeepMind introduced last year. In a demonstration video, a voice says: “Pick up the basketball and slam dunk it” (at 2:27 in the video below). Then a robot arm carefully picks up a miniature basketball and drops it into a miniature net—and while it wasn’t a NBA-level dunk, it was enough to get the DeepMind researchers excited. Google DeepMind released this demo video showing off the capabilities of its Gemini Robotics foundation model to control robots. Gemini Robotics “This basketball example is one of my favorites,” said Kanishka Rao, the principal software engineer for the project, in a press briefing. He explains that the robot had “never, ever seen anything related to basketball,” but that its underlying foundation model had a general understanding of the game, knew what a basketball net looks like, and understood what the term “slam dunk” meant. The robot was therefore “able to connect those [concepts] to actually accomplish the task in the physical world,” says Rao.What are the advances of Gemini Robotics?Carolina Parada, head of robotics at Google DeepMind, said in the briefing that the new models improve over the company’s prior robots in three dimensions: generalization, adaptability, and dexterity. All of these advances are necessary, she said, to create “a new generation of helpful robots.” Generalization means that a robot can apply a concept that it has learned in one context to another situation, and the researchers looked at visual generalization (for example, does it get confused if the color of an object or background changed), instruction generalization (can it interpret commands that are worded in different ways), and action generalization (can it perform an action it had never done before). Parada also says that robots powered by Gemini can better adapt to changing instructions and circumstances. To demonstrate that point in a video, a researcher told a robot arm to put a bunch of plastic grapes into a clear Tupperware container, then proceeded to shift three containers around on the table in an approximation of a shyster’s shell game. The robot arm dutifully followed the clear container around until it could fulfill its directive. Google DeepMind says Gemini Robotics is better than previous models at adapting to changing instructions and circumstances. Google DeepMind As for dexterity, demo videos showed the robotic arms folding a piece of paper into an origami fox and performing other delicate tasks. However, it’s important to note that the impressive performance here is in the context of a narrow set of high-quality data that the robot was trained on for these specific tasks, so the level of dexterity that these tasks represent is not being generalized. What is embodied reasoning?The second model introduced today is Gemini Robotics-ER, with the ER standing for “embodied reasoning,” which is the sort of intuitive physical world understanding that humans develop with experience over time. We’re able to do clever things like look at an object we’ve never seen before and make an educated guess about the best way to interact with it, and this is what DeepMind seeks to emulate with Gemini Robotics-ER.Parada gave an example of Gemini Robotics-ER’s ability to identify an appropriate grasping point for picking up a coffee cup. The model correctly identifies the handle, because that’s where humans tend to grasp coffee mugs. However, this illustrates a potential weakness of relying on human-centric training data: for a robot, especially a robot that might be able to comfortably handle a mug of hot coffee, a thin handle might be a much less reliable grasping point than a more enveloping grasp of the mug itself. DeepMind’s Approach to Robotic Safety Vikas Sindhwani, DeepMind’s head of robotic safety for the project, says the team took a layered approach to safety. It starts with classic physical safety controls that manage things like collision avoidance and stability, but also includes “semantic safety” systems that evaluate both its instructions and the consequences of following them. These systems are most sophisticated in the Gemini Robotics-ER model, says Sindhwani, which is “trained to evaluate whether or not a potential action is safe to perform in a given scenario.”And because “safety is not a competitive endeavor,” Sindhwani says, DeepMind is releasing a new data set and what it calls the Asimov benchmark, which is intended to measure a model’s ability to understand common-sense rules of life. The benchmark contains both questions about visual scenes and text scenarios, asking models’ opinions on things like the desirability of mixing bleach and vinegar (a combination that make chlorine gas) and putting a soft toy on a hot stove. In the press briefing, Sindhwani said that the Gemini models had “strong performance” on that benchmark, and the technical report showed that the models got more than 80 percent of questions correct. DeepMind’s Robotic PartnershipsBack in December, DeepMind and the humanoid robotics company Apptronik announced a partnership, and Parada says that the two companies are working together “to build the next generation of humanoid robots with Gemini at its core.” DeepMind is also making its models available to an elite group of “trusted testers”: Agile Robots, Agility Robotics, Boston Dynamics, and Enchanted Tools.",
    "source": "IEEE Spectrum AI"
  },
  {
    "title": "Microsoft’s Muse AI Edits Video Games on the Fly",
    "url": "https://spectrum.ieee.org/ai-video-games",
    "author": "Matthew S. Smith",
    "date": "2025-04-01",
    "body": "So far, AI has only nipped at the edge of the games industry with tools for art, music, writing, coding, and other elements that make up video games. But what if an AI model could generate examples of gameplay from a single screenshot?That’s the idea behind Microsoft’s Muse, a transformer model with 1.6 billion parameters trained on 500,000 hours of player data. The result is a model that, when prompted with a screenshot of the game, can generate multiple examples of gameplay, which can extend up to several minutes in length.“They have trained what’s essentially a neural game engine that has unprecedented temporal coherence and fidelity,” says Julian Togelius, an associate professor of computer science at New York University and co-founder of AI game testing company Modl.ai. “That has wide implications and is something I could see being used in the future as part of game development more generally.” How Microsoft’s Muse Works Muse (also known as the World and Human Action Model, or WHAM) was trained on human gameplay data from the multiplayer action game Bleeding Edge. The researchers trained a series of models on that data, which varied from 15 million to 1.6 billion parameters; the largest, which performed best, is the focus of a paper published in February in Nature.Though innovative, Muse isn’t the first AI model capable of generating gameplay. Notable predecessors include Google DeepMind’s Genie, Tencent’s GameGen-X, and GameNGen. These earlier models generate visually attractive gameplay and, in many cases, do so at higher frame rates and resolutions than Muse.However, Microsoft’s approach to developing Muse offers several unique advantages.Unlike prior models, Muse was trained on real-world human gameplay data that includes image data from gameplay and corresponding controller inputs. Microsoft was able to access this data through Ninja Theory, a game developer owned by Microsoft’s Xbox Game Studios. Genie and GameGen-X, by contrast, didn’t have access to controller inputs and instead trained on publicly available image data from various games.Muse also uses an autoregressive transformer architecture, which is uncommon for a model that generates images (gameplay, like video, is a series of images in sequence). Muse generates gameplay as sequences of discrete tokens which weave together images and controller actions. While Genie uses a transformer architecture, it doesn’t model controller input. GameNGen and GameGen-X, meanwhile, use specialized diffusion models to generate gameplay, and again don’t model controller input.“What we’ve seen so far, is we haven’t been able to get the consistency with diffusion models that we have with autoregressive models,” says Katja Hofmann, a senior principal research manager at Microsoft Research. The researchers built a frontend called the WHAM Demonstrator to show off the model’s consistency. It can be used to prompt Muse with a screenshot, which then generates multiple “continuations” of gameplay, each providing a different prediction of what might happen. Muse and the WHAM Demonstrator are available for download from HuggingFace. Once generated, users can explore the continuations with a game controller. It’s even possible to drag-and-drop objects the model is familiar with straight into gameplay. The gameplay will update to include the object, which becomes a part of the game world. These objects persisted with a success rate of 85 to 98 percent, depending on the object inserted. Muse users are able to visually tweak the behavior of non-player characters (NPCs) and the environment by drawing directly onto the frame. Image or video references can also be used to influence, and subsequently choose from, scene generations.Anssi Kanervisto, Dave Bignell et al. Building World Models Microsoft’s announcement was careful to avoid calling Muse a complete AI game generator, and for good reason. While its generated gameplay clips are remarkably consistent even across several minutes of gameplay, the clips are generated at a resolution of just 380 by 180 pixels and 10 frames per second, which is far too low for an enjoyable gameplay experience. Muse is also limited to generating gameplay similar to Bleeding Edge.These choices were made to keep Muse manageable; Hofmann says Muse was trained to be “the smallest possible models we can get away with to show what’s possible.” Because of that, she believes there’s room to improve the model’s quality.Instead of pitching itself as a replacement for games, Muse is meant as a tool for developers looking to iterate on gameplay ideas. “You can create a sort of iterative loop. You can create multiple branches of predictions. You can go back, you can make modifications on the fly,” says Hofmann.Muse also represents progress toward creating advanced “world models” that capture the dynamics of a real or simulated environment.Models that generate gameplay, such as Muse and Genie, learn to predict gameplay across multiple modalities that span 3D graphics, 2D graphics, physics, and audio, to name a few. That implies AI models can be trained to form a more general understanding of a complex environment, forming a more wholistic world model rather than an assembly of disparate parts. “In the past, to train a model on something specific, like jazz music, you would need to train to understand music theory, to have many rules and insights,” says Hofmann. “We now have a recipe for training generative AI models on this very complex structured data without a lot of handcrafting of the rules that underlie these systems.”Togelius sees similar possibilities. He says a model like Muse could be used to iterate gameplay not only by generating gameplay, but also by creating world models that simulate an environment. That could in turn open new possibilities for probing and testing that environment, like turning AI agents loose to interact with and learn within the world model.“This has a lot of implications for games, and also for things outside of games,” he says.",
    "source": "IEEE Spectrum AI"
  },
  {
    "title": "It’s Not Just Us: AI Models Struggle With Overthinking",
    "url": "https://spectrum.ieee.org/reasoning-in-ai",
    "author": "Matthew S. Smith",
    "date": "2025-04-01",
    "body": "Recent advances in large language models (LLMs) have drastically improved their ability to reason through answers to prompts. But it turns out that as their ability to reason improves, they increasingly fall victim to a relatable problem: analysis paralysis. A recent preprint paper from a large team, which includes authors from the University of California, Berkeley; ETH Zurich; Carnegie Mellon University; and the University of Illinois Urbana Champaign, found that LLMs with reasoning are prone to overthinking.In other words, the model gets stuck in its own head. What does it mean to overthink? The paper on overthinking, which has not yet been peer reviewed, defines overthinking as “a phenomenon where models favor extended internal reasoning chains over environmental interaction.”Alejandro Cuadrón, a research scholar at UC Berkeley and coauthor on the paper, drew an analogy to the very human problem of decision-making without certainty about the results. “What happens when we really don’t have enough information?” asks Cuadrón. “If you’re asking yourself more and more questions, just talking to yourself…in the best scenario, I’ll realize I need more information. In the worst, I’ll get the wrong results.” To test how the latest AI models handle this situation, Cuadrón and his colleagues tasked leading reasoning LLMs (also known as large reasoning models, or LRMs), such as OpenAI’s o1 and DeepSeek-R1, with solving problems in a popular software-engineering benchmark. The models had to find bugs and design solutions using the OpenHands agentic platform. Cuadrón says the results show a link between a model’s general level of intelligence and its ability to successfully reason through problems.The results? While the best reasoning models performed well overall, reasoning models were found to overthink nearly three times as often as nonreasoning models. And the more a model overthought, the fewer issues it resolved. On average, reasoning models were 7.9 percent less successful per unit increase in overthinking. Reasoning models based on LLMs with relatively few parameters, such as Alibaba’s QwQ-32B (which has 32 billion parameters), were especially prone to overthinking. QwQ, DeepSeek-R1 32B, and Sky-T1-R had the highest overthinking scores, and they weren’t any more successful at resolving tasks than nonreasoning models.Cuadrón says this shows a link between a model’s general level of intelligence and its ability to successfully reason through problems. “I think model size is one of the key contributors, as model size leads to is ‘smartness,’ so to speak,” said Cuadrón. “To avoid overthinking, a model must interact with and understand the environment, and it must understand its output.” Overthinking is an expensive mistake AI overthinking is an intriguing problem from a human perspective, as it mirrors the state of mind we often struggle with. But LLMs are, of course, computer systems, which means overthinking has different consequences. The most obvious is increased compute costs. Reasoning LLMs essentially prompt themselves to reason through a problem, which in turn generates more tokens and keeps expensive hardware (such as GPUs or tensor processing units) occupied. The more reasoning, the higher the costs.Cuadrón and his colleagues found that running OpenAI’s o1 with high reasoning effort could cost as much as US $1,400, whereas a lower-reasoning configuration brought the cost down to $800. Despite that gap, the models performed almost identically on the software-engineering benchmark. OpenAI o1-high resolved 29.1 percent of problems, while o1-low resolved 27.3 percent. The researchers also found that running o1-low multiple times and selecting the best output outperformed o1-high but proved more cost efficient. The lower cost of the low-reasoning model meant this technique saved $200 when compared with o1-high. These results suggest there’s plenty of room to optimize reasoning models, and that throwing more reasoning at a problem isn’t always the best solution. There’s more to think about Interestingly, the paper found that DeepSeek-R1 671B, unlike the other reasoning models tested, didn’t overthink relative to DeepSeek-V3 671B, the nonreasoning model that R1 is based on. That powered R1 to healthy results. It beat DeepSeek-V3 to reach the third-best success rate of all models tested and scored second best among reasoning models.Cuadrón speculates that outcome is due to how DeepSeek trained the model. While large-scale reinforcement learning was key to its training, that technique wasn’t used to train the model specifically for software-engineering tasks. “That means that when the model is presented with a software-engineering task it won’t reason as much, and will prefer to interact with the environment more,” he said.The paper makes a clear argument that LRMs are more efficient when they use only as much reasoning as required to complete a task successfully. But how exactly can a model be trained to use just the right amount of reasoning across a wide variety of tasks? That remains to be solved. The paper’s coauthors hope they can help the broader research community tackle overthinking in LLMs by making their evaluation framework and dataset open source. The full dataset, along with the methodology used to quantify overthinking, is available on GitHub.",
    "source": "IEEE Spectrum AI"
  },
  {
    "title": "The Challenges and Upsides of Using AI in Scientific Writing",
    "url": "https://spectrum.ieee.org/challenges-upsides-ai-scientific-writing",
    "author": "Wynand Lambrechts",
    "date": "2025-04-01",
    "body": "This is a guest post. The views expressed here are the author’s own and do not represent positions of IEEE Spectrum, The Institute or IEEE. Scientific writing is at a pivotal stage, driven by artificial intelligence as a disruptor and enabler. Academics, publishers, and policymakers are attempting to weigh the value of using AI responsibly to enhance productivity versus risking the integrity and purpose of scholarly communication. In this context, the responsible use of the technology in scientific writing pertains to employing AI tools in ways that uphold the integrity, transparency, and ethical standards of scholarly communication.As we collectively contend with the challenges and define AI’s ethical use, we must ask: Is AI revolutionizing scientific writing or undermining it?Technology has long been involved in shaping the scientific writing landscape. Word processors and then personal computers revolutionized how manuscripts were created and shared. The emergence of online submission platforms and open-access repositories further transformed access to knowledge, allowing for large-scale global collaboration and peer review. Modern methods also include alternative metrics that track and analyze the awareness an article generates online to determine where the research is having an impact on social media.From the early digitization of research dissemination to the influence and leverage of social media, challenges to balance progress with quality in writing continue to evolve.AI, especially generative large language models, can draft manuscripts, conduct literature reviews, provide translations, and generate content faster than humans can. The ubiquitous nature and rapid evolution of the advancements, however, require stakeholders to take a step back and consider their ethical and practical limitations and implications.A unified effort within the academic community is needed to ensure that AI in scientific writing is used responsibly to enhance critical thinking, not replace it. This concept aligns with the broader vision of augmented artificial intelligence, advocating for the collaboration between human judgment and AI toward ethical technology development and applying the same principles to scientific writing.Policies and frameworks must stay rooted in the fundamentals of scientific writing: advancing knowledge, prioritizing quality over quantity, and fostering transparency and accountability. Excessive use of AI can have the opposite effect and raise concerns over plagiarism and academic integrity, especially as traditional AI detection algorithms require continuous adaptation to stay relevant. Empirical research lays the foundation for identifying AI’s limitations and refining detection tools to align the technology’s capabilities with ethical standards.Through international collaborative efforts and our shared experiences on the responsible use of AI, we will be able to develop appropriate measures to deal with it in scholarly writing.The challenges AI presents are multidisciplinary and transcend fields. Consistent and collective efforts that address common issues can benefit the entire scientific community.Integrating AI into scientific writingTo integrate AI into scientific writing, we advocate for a collaborative effort to maintain ethical standards, promote transparency, and address challenges that arise. Cooperation among academics, publishers, and policymakers, along with industry and government representatives at both national and international levels, is highly recommended. Such a collaborative effort must strive to create or refine digital identification tools for AI-generated content in academia.We propose that the collaborative effort be underpinned by two primary areas: the development of global frameworks, policies, and training initiatives to encourage the responsible integration of AI in scientific writing and the creation of AI detection tools rooted in empirical research to serve as a blueprint of ethical standards and guidelines.Journals, for example, should regularly update author guidelines to address the evolving role of AI in scientific writing. Clear policies could explicitly permit AI to assist with grammar and language editing while prohibiting its use for drafting original ideas, hypotheses, or results. It also would level the playing field for people who are not native English speakers.To uphold research integrity and align with the core principles of transparency and accountability, AI tools could be allowed to leverage preliminary data analysis, provided that the datasets and methodologies are open-access and reproducible.Manuscript evaluation and editing processes must continuously be refined to detect potential ethical violations and guide the responsible integration of AI into article submissions.Those approaches, combined with updated journal policies, would help set international best practices that augment human intelligence without replacing critical thinking.Global policies, workshops, and training programs must be developed to further uphold the rigorous standards of scholarship within the scientific community. Such collaborative platforms frequently encourage international dialogue and broaden awareness while strengthening the ethical integration of AI in scientific writing. Joint efforts of ethical impact assessments, community engagement initiatives, and global participation promote alignment with best practices, policy coherence, and transparency.To maintain a successful and broad ethical framework, current AI detection tools are vital for identifying AI-generated content and maintaining academic accountability, despite being inconsistent. When combined with human oversight, the tools’ effectiveness and efficacy as first-order checks that verify the responsible use of AI can be increased.As AI detection tools continue to evolve, it is crucial to focus on standardizing and refining the technologies through collaborative global efforts. Even if the scientific writing community does not develop or own its own AI detection systems, its influence and involvement are pivotal in shaping its ethical requirements and guidelines.As the tools become standardized and more accurate, their adoption will assist in maintaining ethical standards in scientific writing and enabling cross-disciplinary stakeholders to address shared challenges. Ways to avoid compromising researchTo address the ethical complexities of AI integration in scientific writing, a comprehensive and proactive approach is essential. Well-defined frameworks and policies, plus adaptive and robust tools, can oversee the responsible use of AI and support collaboration.Key actions to avoid compromising the integrity of scholarly communication include:Encourage cross-disciplinary collaboration to address the ethical challenges of integrating AI into scientific writing.Regularly update and continuously advance journal guidelines to clearly define the permitted and prohibited uses of AI.Implement techniques such as open-access data requirements and improved submission processes to encourage transparency.Support the development of AI detection tools and academic contributions to lay the ethical foundations.Promote global training programs, public engagement, resource-sharing platforms, and international dialogue.The challenges of integrating AI into scientific writing mirror the broader ethical complexities of technological innovation. By prioritizing collaboration, transparency, and accountability, the scientific community can ensure that AI becomes a tool for progress rather than a compromise on existing standards.AI’s transformative power is undeniable. Its integration into scientific writing must be approached with caution and foresight. By prioritizing ethics and quality, academia can navigate the new arena without compromising the foundational principles of scholarly communication and contribution. The ultimate test lies not in how effectively AI can mimic human intelligence but in how responsibly we harness it to uphold the values of scholarship.",
    "source": "IEEE Spectrum AI"
  },
  {
    "title": "AI Solves Million-Step Math Problems",
    "url": "https://spectrum.ieee.org/math-ai",
    "author": "Charles Q. Choi",
    "date": "2025-04-01",
    "body": "Artificial intelligence systems have made breakthrough after breakthrough mastering chess, in which games typically last about 40 moves. Now, to help solve the world’s toughest math problems, researchers have developed a new AI model that finds complex solutions requiring thousands to millions of steps. They suggest the new algorithms they built for the task might one day help detect events such as hurricanes and financial crashes that are rare but have disastrous impacts when they do happen.Scientists are increasingly exploring how well AI can solve math problems. For example, Google DeepMind’s AlphaProof performed as well as a silver medalist in the 2024 International Mathematical Olympiad, a high-school level math competition, and OpenAI’s o3 system recently debuted with a strong showing on benchmark problems in math, science, and computer programming.In a new study, which has not yet been peer reviewed, researchers at the California Institute of Technology and their colleagues tackled more challenging math problems, the kind that have perplexed professional mathematicians for decades.“When it comes to the kind of problems you might find in math olympiads, they’re typically proofs involving 30 or 40 steps, on the same order of magnitude as an average game of chess,” says Sergei Gukov, a professor of theoretical physics and mathematics at the California Institute of Technology, in Pasadena. “We’re focusing on sophisticated research-level math problems with solutions involving thousands or millions or even billions of steps.”Ultimately, “I’m hoping that we’ll be able to solve Millennium Prize problems using AI,” Gukov says, referring to a contest involving the most difficult mathematical problems in the world. “This is probably too optimistic on my part, but it’s good to have some north stars. At the moment, we’re trying to focus on problems one level down instead, the kind that have remained open for many years.”AI Tackles the Andrews-Curtis ConjectureIn the new study, Gukov and his colleagues focused on the Andrews-Curtis conjecture, a combinatorial group theory problem first proposed 60 years ago. “Combinatorial group theory is about transformations of objects,” Gukov says. “Think about a Rubik’s cube. It’s a very simple group with basic operations and transformations—you can rotate different planes of a Rubik’s cube vertically and horizontally. The Andrews-Curtis conjecture is like a Rubik’s cube on steroids—instead of a 3 by 3 by 3 group, it’s more like a 100 by 100 by 100 group.”Although the researchers did not prove the main conjecture, their new system disproved related families of problems known as potential counterexamples that had remained open for about 25 years. These counterexamples are essentially mathematical cases that would disprove the conjecture. Ruling out these counterexamples increases the likelihood that the conjecture is true.To attack these problems, Gukov and his colleagues adopted a strategy where they looked for unexpected, convoluted solutions. “If you were to ask DeepSeek or o3 or ChatGPT or similar models to solve any of the problems we studied, they wouldn’t be able find answers,” he says. “They’re good at producing solutions that are expected or typical, at parroting what’s seen before. They’re meant to be general-purpose models. We’re looking at long sequences of steps that are hard to find, that are outliers among the statistical distribution of solutions.”To develop these “super-moves,” as the researchers call them, Gukov and his colleagues used the approach known as reinforcement learning. They first gave the AI easy problems to solve, and then gave it progressively more difficult problems. The scientists looked for strategies that didn’t require large amounts of computing power; Gukov says that all the training was done on a single GPU.Gukov notes that in reinforcement learning research, people typically use the same 10 to 15 algorithms. “What I find most exciting is that by thinking about solving these problems with these very long horizons, we developed new algorithms for AI,” he says.Potential Applications Beyond MathematicsThese new algorithms “might have many applications for them outside of pure mathematics,” Gukov says. “They can find outliers, anomalies, black swans—events that are very, very rare, but can have very heavy price tags if they do happen.” The rarity of these events has made them difficult for AI to accurately forecast—their unusual nature means there is little historical data for predictive models to learn from. The ability to forecast the most likely and catastrophic of these scenarios could help societies prepare optimal mitigation strategies.Gukov’s team is now investigating other longstanding math problems to help them develop the algorithms. They detailed their findings 13 February in a study on the ArXiv preprint server.",
    "source": "IEEE Spectrum AI"
  },
  {
    "title": "IEEE Offers AI Training Courses and a Mini MBA Program",
    "url": "https://spectrum.ieee.org/ieee-ai-training-mini-mba",
    "author": "Angelique Parashis",
    "date": "2025-04-01",
    "body": "Artificial intelligence is changing the way business is conducted. Organizations that understand where to deploy AI strategically—whether through process improvements, more effective data use, or elsewhere—are expected to outperform their competition and have greater growth and efficiency. In its annual report, “The Impact of Technology in 2025 and Beyond: An IEEE Global Study,” IEEE surveyed 350 global technology leaders including CIOs, CTOs, and IT directors. The survey revealed that more than half of the respondents ranked AI, which encompasses predictive and generative AI, machine learning, and natural language processing, as the most important technology coming into 2025. The tech leaders said they were ready to adopt AI, with many already leveraging its benefits and planning further exploration. Specifically, 20 percent of respondents reported regular use of generative AI, noting that it added value to their operations. Additionally, 24 percent acknowledged the benefits of the technology and said they intended to explore its practical applications. More than 30 percent had high expectations for AI and plan to experiment with it on smaller projects. The strategic importance of AI for companiesAI’s impact varies across industries, with technology leading the way in integration. Despite AI’s increasing presence in company operations around the globe, it remains a source of confusion for most employees. As AI usage continues to rise, businesses should invest in bringing their staff up to speed on how to integrate the technology to improve their operations.In a survey last year by technology research and advisory company Valoir, 84 percent of employees reported being unclear about what generative AI is or how it works. Similarly, in Slingshot’s Digital Work Trends Report, 77 percent of employees surveyed said they didn’t have adequate training in AI tools or fully understand how AI related to their job. The effective use of AI can help companies and their employees make informed, data-driven decisions, improve resource allocation, provide more targeted and personalized customer experiences, and streamline project management. Business leaders who have a firm grasp on what AI can deliver will be better positioned for success. IEEE offers educational resources for AI training For businesses that want to train their staff on the technology, IEEE offers a comprehensive education program designed to enhance knowledge and skills in the rapidly evolving field. The resources, produced by IEEE Educational Activities, can ensure that employees are well-versed in the latest advancements and equipped with practical skills to drive innovation and efficiency within the organization. Created in partnership with IEEE Future Directions, Artificial Intelligence and Machine Learning in Chip Design is a four-hour course series that covers applications in design automation and deployment strategies, as well as the future of design. Developed in partnership with the IEEE Computer Society, Integrating Edge AI and Advanced Nanotechnology in Semiconductor Applications is a five-hour course series that explores the intersection of AI, edge computing, and nanotechnology. The comprehensive AI Integration in Semiconductor Manufacturing five-hour course series, created in partnership with the IEEE Computer Society, covers how AI enhances production efficiency, optimizes processes, and improves product quality. The courses are also available to individuals through the IEEE Learning Network. Upon successfully completing each course, participants earn professional development credits including professional development hours (PDHs) and continuing education units (CEUs). Additionally, they receive a shareable digital badge highlighting their proficiency—which can be showcased on social media platforms. IEEE and Rutgers offer a mini MBAThe new IEEE | Rutgers Online Mini-MBA: Artificial Intelligence program is designed to help organizations and their employees master AI for innovation. The program provides learners with an enhanced understanding of applications tailored to specific industries and job functions. Participants learn how to strategically leverage the technology to address business challenges, optimize processes, make more effective use of data, better serve customer needs, and improve overall organizational success. For employers, the program is invaluable in training staff to stay ahead of the competition in a fast-evolving landscape. It offers individual access and company-specific cohorts, providing flexible learning options to meet your organization’s needs. IEEE members receive a 10 percent discount. Whether you’re an experienced professional or just starting out, IEEE’s education offerings can be invaluable for staying ahead. Learn more about IEEE’s corporate solutions, professional development programs, and individual eLearning courses.",
    "source": "IEEE Spectrum AI"
  },
  {
    "title": "Are You Ready to Let an AI Agent Use Your Computer?",
    "url": "https://spectrum.ieee.org/ai-agents-computer-use",
    "author": "Eliza Strickland",
    "date": "2025-04-01",
    "body": "Two years after the generative AI boom really began with the launch of ChatGPT, it no longer seems that exciting to have a phenomenally helpful AI assistant hanging around in your web browser or phone, just waiting for you to ask it questions. The next big push in AI is for AI agents that can take action on your behalf. But while agentic AI has already arrived for power users like coders, everyday consumers don’t yet have these kinds of AI assistants. That will soon change. Anthropic, Google DeepMind, and OpenAI have all recently unveiled experimental models that can use computers the way people do—searching the web for information, filling out forms, and clicking buttons. With a little guidance from the human user, they can do things like order groceries, call an Uber, hunt for the best price for a product, or find a flight for your next vacation. And while these early models have limited abilities and aren’t yet widely available, they show the direction that AI is going. “This is just the AI clicking around,” said OpenAI CEO Sam Altman in a demo video as he watched the OpenAI agent, called Operator, navigate to OpenTable, look up a San Francisco restaurant, and check for a table for two at 7pm. Zachary Lipton, an associate professor of machine learning at Carnegie Mellon University, notes that AI agents are already being embedded in specialized software for different types of enterprise customers such as salespeople, doctors, and lawyers. But until now, we haven’t seen AI agents that can “do routine stuff on your laptop,” he says. “What’s intriguing here is the possibility of people starting to hand over the keys.”AI Agents from Anthropic, Google DeepMind, and OpenAIAnthropic was the first to unveil this new functionality, with an announcement in October that its Claude chatbot can now “use computers the way humans do.” The company stressed that it was giving the models this capability as a public beta test, and that it’s only available to developers who are building tools and products on top of Anthropic’s large language models. Claude navigates by viewing screenshots of what the user sees and counting the pixels required to move the cursor to a certain spot for a click. A spokesperson for Anthropic says that Claude can do this work on any computer and within any desktop application. Next out of the gate was Google DeepMind with its Project Mariner, built on top of Google’s Gemini 2 language model. The company showed Mariner off in December but called it an “early research prototype” and said it’s only making the tool available to “trusted testers” for now. As another precaution, Mariner currently only operates within the Chrome browser, and only within an active tab, meaning that it won’t run in the background while you work on other tasks. While this requirement seems to somewhat defeat the purpose of having a time-saving AI helper, it’s likely just a temporary condition for this early stage of development. Finally, in January OpenAI launched its computer-use agent (CUA), called Operator. OpenAI called it a “research preview” and made it available only to users who pay US $200 per month for OpenAI’s premium service, though the company said it’s working toward broader release. Yash Kumar, an engineer on the Operator team, says the tool can work with essentially any website. “We’re starting with the browser because this is where the majority of work happens,” Kumar says. But he notes that “the CUA model is also trained to use a computer, so it’s possible we could expand it” to work with other desktop apps. Like the others, Operator relies on chain-of-thought reasoning to take instructions and break them down into a series of tasks that it can complete. If it needs more information to complete a task—like, for example, if you prefer to buy red or yellow onions—it will pause and ask for input. It also asks for confirmation before taking a final step, like booking the restaurant table or putting in the grocery order. Safety Concerns for Computer-Use AgentsHere are some things that computer-use agents can’t yet do: log in to sites, agree to terms of service, solve captchas, and enter credit card or other payment details. If an agent comes up against one of these roadblocks, it hands the steering wheel back to the human user. OpenAI notes that Operator doesn’t take screenshots of the browser while the user is entering login or payment information. The three companies have all noted that putting an AI in charge of your computer could pose safety risks. Anthropic has specifically raised the concern of prompt injection attacks, or ways in which malicious actors can add something to the user’s prompt to make the model take an unexpected action. “Since Claude can interpret screenshots from computers connected to the internet, it’s possible that it may be exposed to content that includes prompt injection attacks,” Anthropic wrote in a blog post. CMU’s Lipton says that the companies haven’t revealed much information about the computer-use agents and how they work, so it’s hard to assess the risks. “If someone is getting your computer operator to do something nefarious, does that mean they already have access to your computer?” he wonders, and if so, why wouldn’t the miscreant just take action directly? Still, Lipton says, with all the actions we take and purchases we make online, “It doesn’t require a wild leap of imagination to imagine actions that would leave the user in a pickle.” For example, he says, “Who will be the first person who wakes up and says, ‘My [agent] bought me a fleet of cars?’”The Future of Computer-Use AgentsWhile none of the companies have revealed a timeline for making their computer-use agents broadly available, it seems likely that consumers will begin to get access to them this year—either through the big AI companies or through startups creating cheaper knockoffs. OpenAI’s Kumar says it’s an exciting time, and that Operator marks a step toward a more collaborative future for humans and AI. “It’s a stepping stone on our path to AGI,” he says, referring to the long-promised dream/nightmare of artificial general intelligence. “The ability to use the same interfaces and tools that humans interact with on a daily basis broadens the utility of AI, helping people save time on everyday tasks.”If you remember the prescient 2013 movie Her, it seems like we’re edging toward the world that existed at the beginning of the film, before the sultry-voiced Samantha began speaking into the protagonist’s ear. It’s a world in which everyone has a boring and neutral AI to help them read and respond to messages and take care of other mundane tasks. Once the AI companies solidly achieve that goal, they’ll no doubt start working on Samantha. This article appears in the April 2025 print issue as “AI Agents Will Soon Take Over Your Computer.”",
    "source": "IEEE Spectrum AI"
  },
  {
    "title": "Brain-inspired Computing Is Ready for the Big Time",
    "url": "https://spectrum.ieee.org/neuromorphic-computing-2671121824",
    "author": "Edd Gent",
    "date": "2025-04-01",
    "body": "Efforts to build brain-inspired computer hardware have been underway for decades, but the field has yet to have its breakout moment. Now, leading researchers say the time is ripe to start building the first large-scale neuromorphic devices that can solve practical problems.The neural networks that have powered recent progress in artificial intelligence are loosely inspired by the brain, demonstrating the potential of technology that takes its cues biology. But the similarities are only skin deep and the algorithms and hardware behind today’s AI operate in fundamentally different ways to biological neurons.Neuromorphic engineers hope that by designing technology that more faithfully replicates the way the brain works, we will be able to mimic both its incredible computing power and its energy efficiency. Central to this approach is the use of spiking neural networks, in which computational neurons mimic their biological cousins by communicating using spikes of activity, rather than the numerical values used in conventional neural networks. But despite decades of research and increasing interest from the private sector, most demonstrations remain small scale and the technology has yet to have a commercial breakout.In a paper published in Nature in January, some of the field’s leading researchers argue this could soon change. Neuromorphic computing has matured from academic prototypes to production-ready devices capable of tackling real-world challenges, they argue, and is now ready to make the leap to large-scale systems. IEEE Spectrum spoke to one of the paper’s authors, Steve Furber, the principal designer of the ARM microprocessor—the technology that now powers most cellphones—and the creator of the SpiNNaker neuromorphic computer architecture.Steve Furber on...Why neuromorphic computing is at a critical junctureSoftware versus hardwareBreaking down research silosScale versus biological plausibilityThe impact of memristorsIn the paper you say that neuromorphic computing is at a critical juncture. What do you mean by that?Steve Furber: We’ve demonstrated that the technology is there to support spiking neural networks at pretty much arbitrary scale and there are useful things that can be done with them. The criticality of the current moment is that we really need some demonstration of a killer app.The SpiNNaker project started 20 years ago with a focus on contributing to brain science, and neuromorphics is an obvious technology if you want to build models of brain cell function. But over the last 20 years, the focus has moved to engineering applications. And to really take off in the engineering space, we need some demonstrations of neuromorphic advantage.In parallel over those 20 years, there’s been an explosion in mainstream AI based on a rather different sort of neural network. And that’s been very impressive and obviously had huge impacts, but it’s beginning to hit some serious problems, particularly in the energy requirements of large language models (LLMs). And there’s now an expectation that neuromorphic approaches may have something to contribute, by significantly reducing those unsustainable energy demands. The SpiNNaker team assembles a million-core neuromorphic system.SpiNNakerWe are close to having neuromorphic systems at a scale sufficient to support LLMs in neuromorphic form. I think there are lots of significant application developments at the smaller end of the spectrum too. Particularly close to sensors, where using something like an event-based image sensor with a neuromorphic processing system could give a very low energy vision system that could be applied in areas such as security and automotive and so on.When you talk about achieving a large-scale neuromorphic computer, how would that compare to systems that already exist?Furber: There are lots of examples out there already like the large Intel Loihi 2 system, Hala Point. That’s a very dense, large-scale system. The SpiNNaker 1 machine that we’ve been running a service on [at the University of Manchester, UK] since 2016 had half a million ARM cores in the system, expanding to a million in 2018. That’s reasonably large scale. Our collaborators on SpiNNaker 2 [SpiNNcloud Systems, based in Dresden, Germany] are beginning to market systems at the 5 million core level, and they will be able to run quite substantial LLMs.Now, how much those will need to evolve for neuromorphic platforms is a question yet to be answered. They can be translated in a fairly simplistic way to get them running, but that simple translation won’t necessarily get the best energy performance.Back to topSo is the hardware not really the issue, it’s working out how to efficiently build something on top of it?Furber: Yes, I think the last 20 years has seen proof-of-concept hardware systems emerge at the scales required. It’s working out how to use them to their best advantage that is the gap. And some of that is simply replicating the efficient and useful software stacks that have been developed for GPU-based machine learning.It is possible to build applications on neuromorphic hardware, but it’s still unreasonably difficult. The biggest missing components are the high-level software design tools along the lines of TensorFlow and PyTorch that make it straightforward to build large models without having to go down to the level of describing every neuron in detail.Back to topThere’s quite a diversity of different neuromorphic technologies, which can sometimes make it hard to translate findings between different groups. How can you break down those silos?Furber: Although the hardware implementation is often quite different, the next level up there is quite a lot in common. All neuromorphic platforms use spiking neurons and the neurons themselves are similar. You have a diversity of details at the lower levels, but that can be bridged by implementing a layer of software that matches those lower level hardware differences to higher level commonalities.We’ve made some progress on that front, because within the EU’s Human Brain Project, we have a group that’s been developing the PyNN language. It is supported by both SpiNNaker, which is a many core neuromorphic system, and the University of Heidelberg’s BrainScaleS system, which is an analog neural model.But it is the case that a lot of neuromorphic systems are developed in a lab and used only by other people within that lab. And therefore they don’t contribute to the drive towards commonality. Intel has been trying to contribute through building the Lava software infrastructure on their Loihi system and encouraging others to participate. So there are moves in that direction but it’s far from complete. A member of the SpiNNaker team checks on the company’s million-core machine.Steve FurberOpinions differ on how biologically plausible neuromorphic technology needs to be. Does the field need to develop some consensus here?Furber: I think the diversity of the hardware platforms and of the neuron models that are used is a strength in the research domain. Diversity is a mechanism for exploring the space and giving you the best chance of finding the best answers for developing serious, large-scale applications. But once you do, yes, I think you need to reduce the diversity and focus more on commonality. So if neuromorphic is about to make the transition from a largely research-driven territory to a largely application-driven territory, then we’d expect to see that kind of thing changing.Back to topIf the field wants to achieve scale will it have to sacrifice a bit of biological plausibility?Furber: There is a trade-off between biological fidelity and engineering controllability. Replicating the extremely simple neural models that are used in LLMs does not require a lot of biological fidelity. Now, it’s arguable that if you could incorporate a bit more of the biological detail and functionality, you could reduce the number of neurons required for those models by a significant factor. If that’s true, then it may well be worth ultimately incorporating those more complex models. But it is still big research problem to prove that this is the case.Back to topIn recent years there’s been a lot of excitement about memristors—memory devices that mimic some of the functionality of neurons. Is that changing the way people are approaching neuromorphic computing?Furber: I do think that the technologies that are being developed have the potential to be transformative in terms of improving hardware efficiency at the very low levels. But when I look at the UK neuromorphic research landscape, a very significant proportion of it is focused on novel device technologies. And arguably, there’s a bit too much focus on that, because the systems problems are the same across the board.Unless we can make progress on the systems level issues it doesn’t really matter what the underpinning technology is, and we already have platforms that will support progress on the systems level issues.The paper suggest that the time is ripe for large-scale neuromorphic computing. What has changed in recent years that makes you positive about this, or is it more a call to arms?Furber: It’s a bit in-between. There is evidence it’s happening, there are a number of interesting startups in the neuromorphic space who are managing to survive. So that’s evidence that people with significant available funds are beginning to be prepared to spend on neuromorphic technology. There’s a belief in the wider community that neuromorphic’s time is coming. And of course, the huge problems facing mainstream machine learning on the energy front, that is a problem which is desperate for a solution. Once there’s a convincing demonstration that neuromorphics can change the equation, then I think we’ll see things beginning to turn.",
    "source": "IEEE Spectrum AI"
  },
  {
    "title": "OpenAI to release open-source model as AI economics force strategic shift",
    "url": "https://venturebeat.com/ai/openai-to-release-open-source-model-as-ai-economics-force-strategic-shift/",
    "author": "Michael Nuñez",
    "date": "2025-04-01",
    "body": "OpenAI plans to release its first open-weight AI model since 2019 as economic pressures mount from competitors like DeepSeek and Meta, marking a significant strategic reversal for the company behind ChatGPT.",
    "source": "VentureBeat AI"
  },
  {
    "title": "$40B into the furnace: As OpenAI adds a million users an hour, the race for enterprise AI dominance hits a new gear",
    "url": "https://venturebeat.com/ai/40b-into-the-furnace-as-openai-adds-a-million-users-an-hour-the-race-for-enterprise-ai-dominance-hits-a-new-gear/",
    "author": "Matt Marshall",
    "date": "2025-04-01",
    "body": "In a move that surprised the tech industry Monday, OpenAI said it has secured a monumental $40 billion funding round led by SoftBank, catapulting its valuation to an unprecedented $300 billion -- making it the largest private equity investment on record. The landmark investment underscores the escalating significance of AI, and also signals a shift in the enterprise technology landscape.",
    "source": "VentureBeat AI"
  },
  {
    "title": "Gartner forecasts gen AI spending to hit $644B in 2025: What it means for enterprise IT leaders",
    "url": "https://venturebeat.com/ai/gartner-forecasts-gen-ai-spending-to-hit-644b-in-2025-what-it-means-for-enterprise-it-leaders/",
    "author": "Sean Michael Kerner",
    "date": "2025-04-01",
    "body": "Gartner forecasts large growth in global AI spending as enterprises shift focus to commercial tools away from custom projects that often fail.",
    "source": "VentureBeat AI"
  },
  {
    "title": "Runway Gen-4 solves AI video’s biggest problem: character consistency across scenes",
    "url": "https://venturebeat.com/ai/runways-gen-4-ai-solves-the-character-consistency-challenge-making-ai-filmmaking-actually-useful/",
    "author": "Michael Nuñez",
    "date": "2025-04-01",
    "body": "Runway's new Gen-4 AI creates consistent characters across entire videos from a single reference image, challenging OpenAI's viral Ghibli trend and potentially transforming how Hollywood makes films.",
    "source": "VentureBeat AI"
  },
  {
    "title": "Game industry leaders go head-to-head in GamesBeat’s Crossfire Lounge",
    "url": "https://venturebeat.com/games/game-industry-leaders-go-head-to-head-in-gamesbeats-crossfire-lounge/",
    "author": "VB Staff",
    "date": "2025-04-01",
    "body": "Three head-to-head conversations, tackling the AI apocalypse, direct-to-player marketing and sales, and the state of the game industry.",
    "source": "VentureBeat AI"
  },
  {
    "title": "Maingear launches MG-1 gaming systems with latest Nvidia 50 Series GPUs",
    "url": "https://venturebeat.com/games/maingear-launches-mg-1-gaming-systems-with-latest-nvidia-50-series-gpus/",
    "author": "Dean Takahashi",
    "date": "2025-04-01",
    "body": "Gaming PC maker Maingear unveiled its new 2025 pre-configured MG-1 gaming systems, equipped with Nvidia's cutting-edge 50 Series GPUs.",
    "source": "VentureBeat AI"
  },
  {
    "title": "CodeScientist: End-to-End Semi-Automated Scientific Discovery with Code-based Experimentation",
    "url": "https://arxiv.org/abs/2503.22708",
    "author": "Peter Jansen, Oyvind Tafjord, Marissa Radensky, Pao Siangliulue, Tom Hope, Bhavana Dalvi Mishra, Bodhisattwa Prasad Majumder, Daniel S. Weld, Peter Clark",
    "date": "2025-04-01",
    "body": "arXiv:2503.22708v1 Announce Type: new Abstract: Despite the surge of interest in autonomous scientific discovery (ASD) of software artifacts (e.g., improved ML algorithms), current ASD systems face two key limitations: (1) they largely explore variants of existing codebases or similarly constrained design spaces, and (2) they produce large volumes of research artifacts (such as automatically generated papers and code) that are typically evaluated using conference-style paper review with limited evaluation of code. In this work we introduce CodeScientist, a novel ASD system that frames ideation and experiment construction as a form of genetic search jointly over combinations of research articles and codeblocks defining common actions in a domain (like prompting a language model). We use this paradigm to conduct hundreds of automated experiments on machine-generated ideas broadly in the domain of agents and virtual environments, with the system returning 19 discoveries, 6 of which were judged as being both at least minimally sound and incrementally novel after a multi-faceted evaluation beyond that typically conducted in prior work, including external (conference-style) review, code review, and replication attempts. Moreover, the discoveries span new tasks, agents, metrics, and data, suggesting a qualitative shift from benchmark optimization to broader discoveries.",
    "source": "Arxiv AI"
  },
  {
    "title": "LLM-based Agent Simulation for Maternal Health Interventions: Uncertainty Estimation and Decision-focused Evaluation",
    "url": "https://arxiv.org/abs/2503.22719",
    "author": "Sarah Martinson, Lingkai Kong, Cheol Woo Kim, Aparna Taneja, Milind Tambe",
    "date": "2025-04-01",
    "body": "arXiv:2503.22719v1 Announce Type: new Abstract: Agent-based simulation is crucial for modeling complex human behavior, yet traditional approaches require extensive domain knowledge and large datasets. In data-scarce healthcare settings where historic and counterfactual data are limited, large language models (LLMs) offer a promising alternative by leveraging broad world knowledge. This study examines an LLM-driven simulation of a maternal mobile health program, predicting beneficiaries' listening behavior when they receive health information via automated messages (control) or live representatives (intervention). Since uncertainty quantification is critical for decision-making in health interventions, we propose an LLM epistemic uncertainty estimation method based on binary entropy across multiple samples. We enhance model robustness through ensemble approaches, improving F1 score and model calibration compared to individual models. Beyond direct evaluation, we take a decision-focused approach, demonstrating how LLM predictions inform intervention feasibility and trial implementation in data-limited settings. The proposed method extends to public health, disaster response, and other domains requiring rapid intervention assessment under severe data constraints. All code and prompts used for this work can be found at https://github.com/sarahmart/LLM-ABS-ARMMAN-prediction.",
    "source": "Arxiv AI"
  },
  {
    "title": "Factored Agents: Decoupling In-Context Learning and Memorization for Robust Tool Use",
    "url": "https://arxiv.org/abs/2503.22931",
    "author": "Nicholas Roth, Christopher Hidey, Lucas Spangher, William F. Arnold, Chang Ye, Nick Masiewicki, Jinoo Baek, Peter Grabowski, Eugene Ie",
    "date": "2025-04-01",
    "body": "arXiv:2503.22931v1 Announce Type: new Abstract: In this paper, we propose a novel factored agent architecture designed to overcome the limitations of traditional single-agent systems in agentic AI. Our approach decomposes the agent into two specialized components: (1) a large language model (LLM) that serves as a high level planner and in-context learner, which may use dynamically available information in user prompts, (2) a smaller language model which acts as a memorizer of tool format and output. This decoupling addresses prevalent issues in monolithic designs, including malformed, missing, and hallucinated API fields, as well as suboptimal planning in dynamic environments. Empirical evaluations demonstrate that our factored architecture significantly improves planning accuracy and error resilience, while elucidating the inherent trade-off between in-context learning and static memorization. These findings suggest that a factored approach is a promising pathway for developing more robust and adaptable agentic AI systems.",
    "source": "Arxiv AI"
  },
  {
    "title": "Identifying Multi-modal Knowledge Neurons in Pretrained Transformers via Two-stage Filtering",
    "url": "https://arxiv.org/abs/2503.22941",
    "author": "Yugen Sato, Tomohiro Takagi",
    "date": "2025-04-01",
    "body": "arXiv:2503.22941v1 Announce Type: new Abstract: Recent advances in large language models (LLMs) have led to the development of multimodal LLMs (MLLMs) in the fields of natural language processing (NLP) and computer vision. Although these models allow for integrated visual and language understanding, they present challenges such as opaque internal processing and the generation of hallucinations and misinformation. Therefore, there is a need for a method to clarify the location of knowledge in MLLMs. In this study, we propose a method to identify neurons associated with specific knowledge using MiniGPT-4, a Transformer-based MLLM. Specifically, we extract knowledge neurons through two stages: activation differences filtering using inpainting and gradient-based filtering using GradCAM. Experiments on the image caption generation task using the MS COCO 2017 dataset, BLEU, ROUGE, and BERTScore quantitative evaluation, and qualitative evaluation using an activation heatmap showed that our method is able to locate knowledge with higher accuracy than existing methods. This study contributes to the visualization and explainability of knowledge in MLLMs and shows the potential for future knowledge editing and control.",
    "source": "Arxiv AI"
  },
  {
    "title": "FindTheFlaws: Annotated Errors for Detecting Flawed Reasoning and Scalable Oversight Research",
    "url": "https://arxiv.org/abs/2503.22989",
    "author": "Gabriel Recchia, Chatrik Singh Mangat, Issac Li, Gayatri Krishnakumar",
    "date": "2025-04-01",
    "body": "arXiv:2503.22989v1 Announce Type: new Abstract: As AI models tackle increasingly complex problems, ensuring reliable human oversight becomes more challenging due to the difficulty of verifying solutions. Approaches to scaling AI supervision include debate, in which two agents engage in structured dialogue to help a judge evaluate claims; critique, in which models identify potential flaws in proposed solutions; and prover-verifier games, in which a capable 'prover' model generates solutions that must be verifiable by a less capable 'verifier'. Evaluations of the scalability of these and similar approaches to difficult problems benefit from datasets that include (1) long-form expert-verified correct solutions and (2) long-form flawed solutions with annotations highlighting specific errors, but few are available. To address this gap, we present FindTheFlaws, a group of five diverse datasets spanning medicine, mathematics, science, coding, and the Lojban language. Each dataset contains questions and long-form solutions with expert annotations validating their correctness or identifying specific error(s) in the reasoning. We evaluate frontier models' critiquing capabilities and observe a range of performance that can be leveraged for scalable oversight experiments: models performing more poorly on particular datasets can serve as judges/verifiers for more capable models. Additionally, for some task/dataset combinations, expert baselines exceed even top model performance, making them more beneficial for scalable oversight experiments.",
    "source": "Arxiv AI"
  },
  {
    "title": "Agentic Large Language Models, a survey",
    "url": "https://arxiv.org/abs/2503.23037",
    "author": "Aske Plaat, Max van Duijn, Niki van Stein, Mike Preuss, Peter van der Putten, Kees Joost Batenburg",
    "date": "2025-04-01",
    "body": "arXiv:2503.23037v1 Announce Type: new Abstract: There is great interest in agentic LLMs, large language models that act as agents. We review the growing body of work in this area and provide a research agenda. Agentic LLMs are LLMs that (1) reason, (2) act, and (3) interact. We organize the literature according to these three categories. The research in the first category focuses on reasoning, reflection, and retrieval, aiming to improve decision making; the second category focuses on action models, robots, and tools, aiming for agents that act as useful assistants; the third category focuses on multi-agent systems, aiming for collaborative task solving and simulating interaction to study emergent social behavior. We find that works mutually benefit from results in other categories: retrieval enables tool use, reflection improves multi-agent collaboration, and reasoning benefits all categories. We discuss applications of agentic LLMs and provide an agenda for further research. Important applications are in medical diagnosis, logistics and financial market analysis. Meanwhile, self-reflective agents playing roles and interacting with one another augment the process of scientific research itself. Further, agentic LLMs may provide a solution for the problem of LLMs running out of training data: inference-time behavior generates new training states, such that LLMs can keep learning without needing ever larger datasets. We note that there is risk associated with LLM assistants taking action in the real world, while agentic LLMs are also likely to benefit society.",
    "source": "Arxiv AI"
  },
  {
    "title": "AstroAgents: A Multi-Agent AI for Hypothesis Generation from Mass Spectrometry Data",
    "url": "https://arxiv.org/abs/2503.23170",
    "author": "Daniel Saeedi, Denise Buckner, Jose C. Aponte, Amirali Aghazadeh",
    "date": "2025-04-01",
    "body": "arXiv:2503.23170v1 Announce Type: new Abstract: With upcoming sample return missions across the solar system and the increasing availability of mass spectrometry data, there is an urgent need for methods that analyze such data within the context of existing astrobiology literature and generate plausible hypotheses regarding the emergence of life on Earth. Hypothesis generation from mass spectrometry data is challenging due to factors such as environmental contaminants, the complexity of spectral peaks, and difficulties in cross-matching these peaks with prior studies. To address these challenges, we introduce AstroAgents, a large language model-based, multi-agent AI system for hypothesis generation from mass spectrometry data. AstroAgents is structured around eight collaborative agents: a data analyst, a planner, three domain scientists, an accumulator, a literature reviewer, and a critic. The system processes mass spectrometry data alongside user-provided research papers. The data analyst interprets the data, and the planner delegates specific segments to the scientist agents for in-depth exploration. The accumulator then collects and deduplicates the generated hypotheses, and the literature reviewer identifies relevant literature using Semantic Scholar. The critic evaluates the hypotheses, offering rigorous suggestions for improvement. To assess AstroAgents, an astrobiology expert evaluated the novelty and plausibility of more than a hundred hypotheses generated from data obtained from eight meteorites and ten soil samples. Of these hypotheses, 36% were identified as plausible, and among those, 66% were novel. Project website: https://astroagents.github.io/",
    "source": "Arxiv AI"
  },
  {
    "title": "Ethereum Price Prediction Employing Large Language Models for Short-term and Few-shot Forecasting",
    "url": "https://arxiv.org/abs/2503.23190",
    "author": "Eftychia Makri, Georgios Palaiokrassas, Sarah Bouraga, Antigoni Polychroniadou, Leandros Tassiulas",
    "date": "2025-04-01",
    "body": "arXiv:2503.23190v1 Announce Type: new Abstract: Cryptocurrencies have transformed financial markets with their innovative blockchain technology and volatile price movements, presenting both challenges and opportunities for predictive analytics. Ethereum, being one of the leading cryptocurrencies, has experienced significant market fluctuations, making its price prediction an attractive yet complex problem. This paper presents a comprehensive study on the effectiveness of Large Language Models (LLMs) in predicting Ethereum prices for short-term and few-shot forecasting scenarios. The main challenge in training models for time series analysis is the lack of data. We address this by leveraging a novel approach that adapts existing pre-trained LLMs on natural language or images from billions of tokens to the unique characteristics of Ethereum price time series data. Through thorough experimentation and comparison with traditional and contemporary models, our results demonstrate that selectively freezing certain layers of pre-trained LLMs achieves state-of-the-art performance in this domain. This approach consistently surpasses benchmarks across multiple metrics, including Mean Squared Error (MSE), Mean Absolute Error (MAE), and Root Mean Squared Error (RMSE), demonstrating its effectiveness and robustness. Our research not only contributes to the existing body of knowledge on LLMs but also provides practical insights in the cryptocurrency prediction domain. The adaptability of pre-trained LLMs to handle the nature of Ethereum prices suggests a promising direction for future research, potentially including the integration of sentiment analysis to further refine forecasting accuracy.",
    "source": "Arxiv AI"
  },
  {
    "title": "GRASP: Municipal Budget AI Chatbots for Enhancing Civic Engagement",
    "url": "https://arxiv.org/abs/2503.23299",
    "author": "Jerry Xu, Justin Wang, Joley Leung, Jasmine Gu",
    "date": "2025-04-01",
    "body": "arXiv:2503.23299v1 Announce Type: new Abstract: There are a growing number of AI applications, but none tailored specifically to help residents answer their questions about municipal budget, a topic most are interested in but few have a solid comprehension of. In this research paper, we propose GRASP, a custom AI chatbot framework which stands for Generation with Retrieval and Action System for Prompts. GRASP provides more truthful and grounded responses to user budget queries than traditional information retrieval systems like general Large Language Models (LLMs) or web searches. These improvements come from the novel combination of a Retrieval-Augmented Generation (RAG) framework (\"Generation with Retrieval\") and an agentic workflow (\"Action System\"), as well as prompt engineering techniques, the incorporation of municipal budget domain knowledge, and collaboration with local town officials to ensure response truthfulness. During testing, we found that our GRASP chatbot provided precise and accurate responses for local municipal budget queries 78% of the time, while GPT-4o and Gemini were only accurate 60% and 35% of the time, respectively. GRASP chatbots greatly reduce the time and effort needed for the general public to get an intuitive and correct understanding of their town's budget, thus fostering greater communal discourse, improving government transparency, and allowing citizens to make more informed decisions.",
    "source": "Arxiv AI"
  },
  {
    "title": "LaViC: Adapting Large Vision-Language Models to Visually-Aware Conversational Recommendation",
    "url": "https://arxiv.org/abs/2503.23312",
    "author": "Hyunsik Jeon, Satoshi Koide, Yu Wang, Zhankui He, Julian McAuley",
    "date": "2025-04-01",
    "body": "arXiv:2503.23312v1 Announce Type: new Abstract: Conversational recommender systems engage users in dialogues to refine their needs and provide more personalized suggestions. Although textual information suffices for many domains, visually driven categories such as fashion or home decor potentially require detailed visual information related to color, style, or design. To address this challenge, we propose LaViC (Large Vision-Language Conversational Recommendation Framework), a novel approach that integrates compact image representations into dialogue-based recommendation systems. LaViC leverages a large vision-language model in a two-stage process: (1) visual knowledge self-distillation, which condenses product images from hundreds of tokens into a small set of visual tokens in a self-distillation manner, significantly reducing computational overhead, and (2) recommendation prompt tuning, which enables the model to incorporate both dialogue context and distilled visual tokens, providing a unified mechanism for capturing textual and visual features. To support rigorous evaluation of visually-aware conversational recommendation, we construct a new dataset by aligning Reddit conversations with Amazon product listings across multiple visually oriented categories (e.g., fashion, beauty, and home). This dataset covers realistic user queries and product appearances in domains where visual details are crucial. Extensive experiments demonstrate that LaViC significantly outperforms text-only conversational recommendation methods and open-source vision-language baselines. Moreover, LaViC achieves competitive or superior accuracy compared to prominent proprietary baselines (e.g., GPT-3.5-turbo, GPT-4o-mini, and GPT-4o), demonstrating the necessity of explicitly using visual data for capturing product attributes and showing the effectiveness of our vision-language integration. Our code and dataset are available at https://github.com/jeon185/LaViC.",
    "source": "Arxiv AI"
  },
  {
    "title": "Automating Artificial Life Discovery: The Power of Foundation Models",
    "url": "https://syncedreview.com/2024/12/31/self-evolving-prompts-redefining-ai-alignment-with-deepmind-chicago-us-eva-framework-19/",
    "author": "Synced",
    "date": "2025-04-01",
    "body": "A research team introduces Automated Search for Artificial Life (ASAL). This novel framework leverages vision-language FMs to automate and enhance the discovery process in ALife research. The post Automating Artificial Life Discovery: The Power of Foundation Models first appeared on Synced.",
    "source": "Synced AI"
  },
  {
    "title": "Llama 3 Meets MoE: Pioneering Low-Cost High-Performance AI",
    "url": "https://syncedreview.com/2024/12/28/self-evolving-prompts-redefining-ai-alignment-with-deepmind-chicago-us-eva-framework-18/",
    "author": "Synced",
    "date": "2025-04-01",
    "body": "Researchers from the University of Texas at Austin and NVIDIA proposes upcycling approach, an innovative training recipe enables the development of an 8-Expert Top-2 MoE model using Llama 3-8B with less than 1% of the compute typically required for pre-training. The post Llama 3 Meets MoE: Pioneering Low-Cost High-Performance AI first appeared on Synced.",
    "source": "Synced AI"
  },
  {
    "title": "DeepMind’s JetFormer: Unified Multimodal Models Without Modelling Constraints",
    "url": "https://syncedreview.com/2024/12/26/self-evolving-prompts-redefining-ai-alignment-with-deepmind-chicago-us-eva-framework-17/",
    "author": "Synced",
    "date": "2025-04-01",
    "body": "A DeepMind research team introduces JetFormer, a Transformer designed to directly model raw data. This model maximizes the likelihood of raw data without depending on any pre-trained components, and is capable of both understanding and generating text and images seamlessly. The post DeepMind’s JetFormer: Unified Multimodal Models Without Modelling Constraints first appeared on Synced.",
    "source": "Synced AI"
  },
  {
    "title": "NVIDIA’s nGPT: Revolutionizing Transformers with Hypersphere Representation",
    "url": "https://syncedreview.com/2024/12/23/self-evolving-prompts-redefining-ai-alignment-with-deepmind-chicago-us-eva-framework-16/",
    "author": "Synced",
    "date": "2025-04-01",
    "body": "An NVIDIA research team proposes the normalized Transformer, which consolidates key findings in Transformer research under a unified framework, offering faster learning and reduced training steps—by factors ranging from 4 to 20 depending on sequence length. The post NVIDIA’s nGPT: Revolutionizing Transformers with Hypersphere Representation first appeared on Synced.",
    "source": "Synced AI"
  },
  {
    "title": "From Token to Conceptual: Meta introduces Large Concept Models in Multilingual AI",
    "url": "https://syncedreview.com/2024/12/17/self-evolving-prompts-redefining-ai-alignment-with-deepmind-chicago-us-eva-framework-15/",
    "author": "Synced",
    "date": "2025-04-01",
    "body": "A research team at Meta introduces the Large Concept Model (LCM), a novel architecture that processes input at a higher semantic level. This shift allows the LCM to achieve remarkable zero-shot generalization across languages, outperforming existing LLMs of comparable size. The post From Token to Conceptual: Meta introduces Large Concept Models in Multilingual AI first appeared on Synced.",
    "source": "Synced AI"
  },
  {
    "title": "NVIDIA’s Hybrid: Combining Attention and State Space Models for Breakthrough Performance of Small Language Models",
    "url": "https://syncedreview.com/2024/12/14/self-evolving-prompts-redefining-ai-alignment-with-deepmind-chicago-us-eva-framework-14/",
    "author": "Synced",
    "date": "2025-04-01",
    "body": "An NVIDIA research team proposes Hymba, a family of small language models that blend transformer attention with state space models, which outperforms the Llama-3.2-3B model with a 1.32% higher average accuracy, while reducing cache size by 11.67× and increasing throughput by 3.49×. The post NVIDIA’s Hybrid: Combining Attention and State Space Models for Breakthrough Performance of Small Language Models first appeared on Synced.",
    "source": "Synced AI"
  },
  {
    "title": "From Response to Query: The Power of Reverse Thinking in Language Models",
    "url": "https://syncedreview.com/2024/12/12/self-evolving-prompts-redefining-ai-alignment-with-deepmind-chicago-us-eva-framework-13/",
    "author": "Synced",
    "date": "2025-04-01",
    "body": "In a new paper Time-Reversal Provides Unsupervised Feedback to LLMs, a research team from Google DeepMind and Indian Institute of Science proposes Time Reversed Language Models (TRLMs), a framework that allows LLMs to reason in reverse—scoring and generating content in a manner opposite to the traditional forward approach. The post From Response to Query: The Power of Reverse Thinking in Language Models first appeared on Synced.",
    "source": "Synced AI"
  },
  {
    "title": "Yann LeCun Team’s New Research: Revolutionizing Visual Navigation with Navigation World Models",
    "url": "https://syncedreview.com/2024/12/09/self-evolving-prompts-redefining-ai-alignment-with-deepmind-chicago-us-eva-framework-12/",
    "author": "Synced",
    "date": "2025-04-01",
    "body": "In a new paper Navigation World Models, a research team from Meta, New York University and Berkeley AI Research proposes a Navigation World Model (NWM), a controllable video generation model that enables agents to simulate potential navigation plans and assess their feasibility before taking action. The post Yann LeCun Team’s New Research: Revolutionizing Visual Navigation with Navigation World Models first appeared on Synced.",
    "source": "Synced AI"
  },
  {
    "title": "The Future of Vision AI: How Apple’s AIMV2 Leverages Images and Text to Lead the Pack",
    "url": "https://syncedreview.com/2024/12/07/self-evolving-prompts-redefining-ai-alignment-with-deepmind-chicago-us-eva-framework-11/",
    "author": "Synced",
    "date": "2025-04-01",
    "body": "An Apple research team introduces AIMV2, a family of vision encoders that is designed to predict both image patches and text tokens within a unified sequence. This combined objective enables the model to excel in a range of tasks, such as image recognition, visual grounding, and multimodal understanding. The post The Future of Vision AI: How Apple’s AIMV2 Leverages Images and Text to Lead the Pack first appeared on Synced.",
    "source": "Synced AI"
  },
  {
    "title": "Redefining Music AI: The Power of Sony’s SoniDo as a Versatile Foundation Model",
    "url": "https://syncedreview.com/2024/12/05/self-evolving-prompts-redefining-ai-alignment-with-deepmind-chicago-us-eva-framework-10/",
    "author": "Synced",
    "date": "2025-04-01",
    "body": "In a new paper Music Foundation Model as Generic Booster for Music Downstream Tasks, a Sony research team presents SoniDo, a groundbreaking music foundation model that offers robust framework for improving the effectiveness and accessibility of music processing. The post Redefining Music AI: The Power of Sony’s SoniDo as a Versatile Foundation Model first appeared on Synced.",
    "source": "Synced AI"
  }
]